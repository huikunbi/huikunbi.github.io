<!doctype html>
<html>
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Huikun Bi </title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <p><small><a href="https://huikunbi.github.io/index.html">English</a> | <a href="https://huikunbi.github.io/index_ch.html">中文</a></small></p>
        <h1>毕慧堃</h1>
        <img src="research/WeChat Image_20181109174303.jpg" height="180" width="180">
        <p><br>在读博士生 <br>计算技术研究所,中国科学院, <br>北京,中国</br></p>
<!--         <p>Research Affiliate<br><a href="http://legacy.iza.org/en/webcontent/personnel/photos/index_html?key=24155">Institute for the Study of Labor (IZA)</a></p> -->
<!--     <h3><p class="view"><a href="https://huikunbi.github.io/">Home</a></p></h3> -->
<!--         <h3><p class="view"><a href="https://huikunbi.github.io/index.html">Research</a></p></h3> -->
    <h3><p class="view"><a href="https://huikunbi.github.io/research/CV.pdf"><br>个人简历</a></p></h3>  
<!--         <h3><p class="view"><a href="https://huikunbi.github.io/code.html">Code</a></p></h3> 
        <h3><p class="view"><a href="https://huikunbi.github.io/teaching.html">Teaching</a></p></h3> 
        <h3><p class="view"><a href="https://huikunbi.github.io/personal.html">Personal</a></p></h3> -->
    <p class="view"><b>Social</b><br>
        <a href="mailto:xiaobi361@gmail.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a> xiaobi361@gmail.com<br>
<!--         <a href="https://scholar.google.com/citations?user=eohlTTcAAAAJ&hl=en" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>
        <a href="https://orcid.org/0000-0002-6910-0363"><i class="ai ai-fw ai-orcid-square"></i> ORCID</a><br>
        <a href="http://ideas.repec.org/f/pra541.html"><i class="fa fa-fw fa-share-alt-square"></i> RePEc</a><br>
        <a href="http://github.com/huikunbi"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
        <a href="http://twitter.com/huikunbi" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a><br> -->
        <a href="http://linkedin.com/in/huikunbi" class="author-social" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a><br>

    <p><b>联系方式:</b><br>中国科学院 计算技术研究所,<br>中关村科学院南路6号,<br>海淀区,北京,100190,中国</p>
      </header>
      <section>

<!--     <h3><b>自我介绍</b></h3> -->
      <p>
        我是中国科学院计算技术研究所六年级在读博士生（硕博连读）。我的博士生指导老师是王兆其研究员和毛天露副研究员。
          
      </p>
          
      <p>
        我的研究方向是计算机图形学和计算机视觉，特别是可视化交通仿真，基于深度学习的轨迹预测和自动驾驶。
          
      </p>
          
      <p>
        在2015.12-2016.04和2016.12-2018.12期间，我在美国休斯顿大学，跟从邓志刚教授交流访问。
      </p>
        <hr>
<!--  -------------------------------------------------------        -->

   <h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a><b>研究论文</b></h2>
      <!--  -----------------------------------------------------------------     -->
       
                     <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/.pdf">
          How Can I See My Future? FvTraj: Using First-person View for Pedestrian Trajectory Prediction</a> 
          <br><b> Huikun Bi</b>, Ruisi Zhang, Tianlu Mao, Zhigang Deng, and Zhaoqi Wang. 
          <br>European Conference on Computer Vision (ECCV), 2020, accepted, to be presented.
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/.pdf">paper</a> 
      ]
          
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
In this paper, we present a novel First-person View based Trajectory predicting model (FvTraj) to estimate the future trajectories of pedestrians in a scene given their observed trajectories and the corresponding first-person view images. First, we generate first-person view images using our in-house built First-person View image Simulator (FvSim), given the observed trajectories. Then, based on multi-head attention mechanisms, we design a social-aware attention module to model social interactions between pedestrians, and a view-aware attention module to capture the relations between historical motion states and visual features from the first-person view images. Our results show the first-person view information could successfully capture the detailed dynamic scene contexts with ego-motions. Using our simulated first-person view images, our well structured FvTraj model achieves state-of-the-art performance.        </div></p>
<!--     <img src="research/2018tvcg.jpg" height="160" width="480">  -->
  </p> 
        

        
        
        
        
        <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/A_Deep_Learning_based_Framework_for_Intersectional_Traffic_Simulation_and_Editing.pdf">
          A Deep Learning-based Framework for Intersectional Traffic Simulation and Editing</a> 
          <br><b> Huikun Bi</b>, Tianlu Mao, Zhaoqi Wang, and Zhigang Deng. 
          <br>IEEE Transactions on Visualization and Computer Graphics (TVCG), vol. 26, no. 7, pp. 2335-2348, 1 July 2020 
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/A_Deep_Learning_based_Framework_for_Intersectional_Traffic_Simulation_and_Editing.pdf">paper</a> 
      ]
      [ <a style="margin:0; font-size:100%; font-weight:bold" href="https://youtu.be/WvOS9spBZ-0">video</a>
      ]     
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
Most of existing traffic simulation methods have been focused on simulating vehicles on freeways or city-scale urban networks. However, relatively little research has been done to simulate intersectional traffic to date despite its obvious importance in real-world traffic phenomena. In this paper we propose a novel deep learning-based framework to simulate and edit intersectional traffic. Specifically, based on an in-house collected intersectional traffic dataset, we employ the combination of convolution network (CNN) and recurrent network (RNN) to learn the patterns of vehicle trajectories in intersectional traffic. Besides simulating novel intersectional traffic, our method can be used to edit existing intersectional traffic. Through many experiments as well as comparison user studies, we demonstrate that the results by our method are visually indistinguishable from ground truth and perform better than other methods.
        </div></p>
<!--     <img src="research/2018tvcg.jpg" height="160" width="480">  -->
  </p>
        
        
        <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="---.pdf">
          Vehicle Trajectory Prediction Using LSTMs with Spatial-Temporal Attention Mechanisms</a> 
          <br>Lei Lin, Weizi Li, <b> Huikun Bi </b>, and Lingqiao Qin. 
          <br>IEEE Intelligent Transportation Systems Magazine (ITSM), 2020, accepted
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="---.pdf">paper</a> 
      ]
        
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
Accurate vehicle trajectory prediction can benefit many Intelligent Transportation System (ITS) applications such as traffic simulation and advanced driver assistance system. This ability is pronounced with the emergence of autonomous vehicles, as they require the prediction of nearby agents' trajectories to navigate safely and efficiently. Recent studies based on deep learning have greatly improved prediction accuracy. However, one prominent issue is that these models often lack explainability. We alleviate this issue by proposing STA-LSTM, an LSTM model with spatial-temporal attention mechanisms. STA-LSTM not only outperforms other state-of-the-art models in prediction accuracy but also identifies the influence of historical trajectories and neighboring vehicles on the target vehicle via spatial-temporal attention weights. We provide analyses of the learned attention weights in various traffic scenarios based on target vehicle class, target vehicle location, and traffic density. An analysis showing that STA-LSTM can capture fine-grained lane-changing behaviors is also provided. </div></p>
<!--     <img src="research/EG_survey.png.jpg" height="180" width="480">  -->
  </p>       
  <!--  -----------------------------------------------------------------     -->

        <!--  -----------------------------------------------------------------     -->
        <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/2020HB-colgan.PDF">
          CoL-GAN: Plausible and Collision-less Trajectory Prediction by Attention-based GAN</a> 
          <br>Shaohua Liu, Haibo Liu, <b> Huikun Bi </b>, and Tianlu Mao. 
          <br>IEEE Xplore Early Access, 2020
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/2020HB-colgan.PDF">paper</a> 
      ]
        
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
Predicting plausible and collisionless trajectories is critical in various applications, such as robotic navigation and autonomous driving. This is a challenging task due to two major factors. First, it is difficult for deep neural networks to understand how pedestrians move to avoid collisions and how they react to each other. Second, given observed trajectories, there are multiple possible and plausible trajectories followed by pedestrians. Although an increasing number of previous works have focused on modeling social interactions and multimodality, the trajectories generated by these methods still lead to many collisions. In this work, we propose CoL-GAN, a new attention-based generative adversarial network using a convolutional neural network as a discriminator, which is able to generate trajectories with fewer collisions. Through experimental comparisons with prior works on publicly available datasets, we demonstrate that Col-GAN achieves state-of-the-art performance in terms of accuracy and collision avoidance.</div></p>
<!--     <img src="research/EG_survey.png.jpg" height="180" width="480">  -->
  </p>       
  <!--  -----------------------------------------------------------------     -->
        
        
        <!--  -----------------------------------------------------------------     -->
        <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes.pdf">
          Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes</a> 
          <br><b> Huikun Bi </b>, Zhong Fang, Tianlu Mao, Zhaoqi Wang, Zhigang Deng. 
          <br>International Conference in Computer Vision (ICCV), 2019
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="http://vr.ict.ac.cn/vp-lstm">project page</a> 
      ]
          [ <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes.pdf">paper</a> 
      ]
        [ <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes-supp.pdf">supplemental</a> 
      ]
          [ <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.youtube.com/watch?v=AlBiVGr0Cw4&t=2s">video</a> 
      ]
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
Trajectory prediction for objects is challenging and critical for various applications (e.g., autonomous driving, and anomaly detection). Most of the existing methods focus on homogeneous pedestrian trajectories prediction, where pedestrians are treated as particles without size. However, they fall short of handling crowded vehicle-pedestrian-mixed scenes directly since vehicles, limited with kinematics in reality, should be treated as rigid, non-particle objects ideally. In this paper, we tackle this problem using separate LSTMs for heterogeneous vehicles and pedestrians. Specifically, we use an oriented bounding box to represent each vehicle, calculated based on its position and orientation, to denote its kinematic trajectories. We then propose a framework called VP-LSTM to predict the kinematic trajectories of both vehicles and pedestrians simultaneously. In order to evaluate our model, a large dataset containing the trajectories of both vehicles and pedestrians in vehicle-pedestrian-mixed scenes is specially built. Through comparisons between our method with state-of-the-art approaches, we show the effectiveness and advantages of our method on kinematic trajectories prediction in vehicle-pedestrian-mixed scenes.</div></p>
<!--     <img src="research/EG_survey.png.jpg" height="180" width="480">  -->
  </p>       
  <!--  -----------------------------------------------------------------     -->
        
        
        <!--  -----------------------------------------------------------------     -->
        <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/2019ICCV_STGAT.pdf">
          STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction</a> 
          <br>Yingfan Huang, <b> Huikun Bi </b>, Zhaoxin Li , Tianlu Mao, Zhaoqi Wang. 
          <br>International Conference in Computer Vision (ICCV), 2019, oral presentation  
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/2019ICCV_STGAT.pdf">paper</a> 
      ]
        
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
Human trajectory prediction is challenging and critical in various applications (e.g., autonomous vehicles and social robots). Because of the continuity and foresight of the pedestrian movements, the moving pedestrians in crowded spaces will consider both spatial and temporal interactions to avoid future collisions. However, most of the existing methods ignore the temporal correlations of interactions with other pedestrians involved in a scene. In this work, we propose a Spatial-Temporal Graph Attention network (STGAT), based on a sequence-to-sequence architecture to predict future trajectories of pedestrians. Besides the spatial interactions captured by the graph attention mechanism at each time-step, we adopt an extra LSTM to encode the temporal correlations of interactions. Through comparisons with state-of-the-art methods, our model achieves superior performance on two publicly available crowd datasets (ETH and UCY) and produces more "socially" plausible trajectories for pedestrians.</div></p>
<!--     <img src="research/EG_survey.png.jpg" height="180" width="480">  -->
  </p>       
  <!--  -----------------------------------------------------------------     -->
        
        
        
        <!--  -----------------------------------------------------------------     -->
        <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/2019 EG_STAR___A_Survey_on_Visual_Traffic_Simulation_and_Animation.pdf">
          A Survey on Visual Traffic Simulation: Models, Evaluations, and Applications in Autonomous Driving</a> 
          <br>Qianwen Chao*, <b> Huikun Bi* (* indicates equal contribution)</b>, Weizi Li, Tianlu Mao, Zhaoqi Wang, Ming C. Lin, Zhigang Deng. 
          <br>Computer Graphics Forum (CGF), 2019  
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/2019 EG_STAR___A_Survey_on_Visual_Traffic_Simulation_and_Animation.pdf">paper</a> 
      ]
        
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
Virtualized traffic via various simulation models and real-world traffic data are promising approaches to reconstruct detailed traffic flows. A variety of applications can benefit from the virtual traffic, including, but not limited to, video games, virtual reality, traffic engineering, and autonomous driving. In this survey, we provide a comprehensive review on the state-of-the-art techniques for traffic simulation and animation. We start with a discussion on three classes of traffic simulation models applied at different levels of detail. Then, we introduce various data-driven animation techniques, including existing data collection methods, and the validation and evaluation of simulated traffic flows. Next, We discuss how traffic simulations can benefit the training and testing of autonomous vehicles. Finally, we discuss the current states of traffic simulation and animation and suggest future research directions.</div></p>
<!--     <img src="research/EG_survey.png.jpg" height="180" width="480">  -->
  </p>       
  <!--  -----------------------------------------------------------------     -->
<!--  -----------------------------------------------------------------     -->

 <!--  -----------------------------------------------------------------     -->
  <p>
        <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/SCA2016_lanechanging.pdf">
          A Data-driven Model for Lane-changing in Traffic Simulation</a> 
          <br><b> Huikun Bi</b>, Tianlu Mao, Zhaoqi Wang, Zhigang Deng. 
<!--           <br> <i>Eurographics/ACM SIGGRAPH Symposium on Computer Animation,</i>2016  -->
          <br>Eurographics/ACM SIGGRAPH Symposium on Computer Animation (SCA), 2016 
      
      <br>[ <a style="margin:0; font-size:100%; font-weight:bold" href="https://huikunbi.github.io/research/SCA2016_lanechanging.pdf">paper</a> 
      ]
      [ <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.youtube.com/watch?v=qPqpmwZxM4s&feature=youtu.be">video</a>
      ]     
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> In this paper, we propose a new data-driven model to simulate the process of lane-changing in traffic simulation. Specifically, we first extract the features from surrounding vehicles that are relevant to the lane-changing of the subject vehicle. Then, we learn the lane-changing characteristics from the ground-truth vehicle trajectory data using randomized forest and back-propagation neural network algorithms. Our method can make the subject vehicle to take account of more gap options on the target lane to cut in as well as achieve more realistic lane-changing trajectories for the subject vehicle and the follower vehicle. Through many experiments and comparisons with selected state-of-the-art methods, we demonstrate that our approach can soundly outperform them in terms of the accuracy and quality of lane-changing simulation. Our model can be flexibly used together with a variety of existing car-following models to produce natural traffic animations in various virtual environments. </div></p>
<!--     <img src="research/sca.jpg" height="160" width="480"> -->
 </p>
 <!--  -----------------------------------------------------------------     -->
    
</section>
      
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
